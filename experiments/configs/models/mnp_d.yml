model:
  _target_: tnp.models.mnp.MNP
  encoder: ${mnp_encoder}
  decoder: ${mnp_decoder}
  likelihood: ${likelihood}

mnp_encoder:
  _target_: tnp.models.mnp.MNPEncoder
  mamba_encoder: ${mamba_encoder}
  xy_encoder: ${xy_encoder}
  # x_encoder and y_encoder default to Identity, which is standard

mamba_encoder:
  _target_: tnp.networks.mamba.MNPDMambaEncoder
  num_layers: ${params.num_layers}
  mamba_layer: ${mamba_layer}

mamba_layer:
  # Defined in mamba_layers.py
  _target_: tnp.networks.mamba_layers.MambaEncoderLayer
  embed_dim: ${params.embed_dim}
  norm: True
  residual: False            
  mamba2: False              
  enc_conv: False            
  enc_conv_kernel: 5         
  enc_conv_dilation: 0       
  d_state: 128               
  block_expansion: 2         
  d_conv: 4                  
  bidirectional_mamba: False

xy_encoder:
  _target_: tnp.networks.mlp.MLP
  # Input dim is typically 1 (mask/density) + dim_y + dim_x
  in_dim: ${eval:'1 + ${params.dim_y} + ${params.dim_x}'}
  out_dim: ${params.embed_dim}
  num_layers: 2
  width: ${params.embed_dim}

mnp_decoder:
  _target_: tnp.models.mnp.MNPDecoder
  z_decoder: ${z_decoder}

z_decoder:
  _target_: tnp.networks.mlp.MLP
  in_dim: ${params.embed_dim}
  out_dim: ${eval:'2 * ${params.dim_y}'}
  num_layers: 2
  width: ${params.embed_dim}

likelihood:
  _target_: tnp.likelihoods.gaussian.HeteroscedasticNormalLikelihood

optimiser:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 5.0e-4

params:
  epochs: 200
  embed_dim: 128
  num_heads: 8 # Kept as parameter variable, though unused by MNPDMambaEncoder
  head_dim: 16
  norm_first: True
  num_layers: 5

misc:
  name: MNP-D tests
  resume_from_checkpoint: null
  gradient_clip_val: 0.5
  plot_interval: 10