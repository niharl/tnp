import copy
import warnings
from abc import ABC
from typing import Optional

import einops
import torch
from check_shapes import check_shapes
from torch import nn
from mamba_ssm import Mamba, Mamba2
from .convolutions import DilatedConv1dBlock

class MambaEncoderLayer(nn.Module):
    """
    Code based on: https://github.com/automl/Mamba4Cast/blob/main/src_torch/training/blocks.py
    """
    def __init__(
            self, 
            embed_dim, 
            norm=True, 
            residual=False, # Not implemented currently
            mamba2=False, 
            enc_conv=False, # Not implemented currently 
            bidirectional_mamba=False,
            enc_conv_kernel=5, 
            enc_conv_dilation=0, 
            d_state=128, 
            block_expansion=2, 
            d_conv=4,
            layer_idx=None,
            **kwargs
    ):
        super().__init__(**kwargs)
        
        self.enc_conv = enc_conv
        self.norm = norm
        self.bidirectional_mamba = bidirectional_mamba
        self.layer_idx = layer_idx

        if not mamba2:
            self.mamba_layer = Mamba(
                # This module uses roughly 3 * expand * d_model^2 parameters
                d_model=embed_dim, # Model dimension d_model
                d_state=d_state,  # SSM state expansion factor
                d_conv=d_conv,    # Local convolution width
                expand=block_expansion,    # Block expansion factor
            )

        else:
            self.mamba_layer = Mamba2(
                d_model=embed_dim, # Model dimension d_model
                d_state=d_state,  # SSM state expansion factor
                d_conv=d_conv,    # Local convolution width
                expand=block_expansion,    # Block expansion factor
            )

        if self.bidirectional_mamba:
            if not mamba2:
                self.mamba_layer_backward = Mamba(
                    # This module uses roughly 3 * expand * d_model^2 parameters
                    d_model=embed_dim, # Model dimension d_model
                    d_state=d_state,  # SSM state expansion factor
                    d_conv=d_conv,    # Local convolution width
                    expand=block_expansion,    # Block expansion factor
                )
            else:
                self.mamba_layer_backward = Mamba2(
                    d_model=embed_dim, # Model dimension d_model
                    d_state=d_state,  # SSM state expansion factor
                    d_conv=d_conv,    # Local convolution width
                    expand=block_expansion,    # Block expansion factor
                )
            
        if self.enc_conv:
            # Optional convolutional layer instead of feed-forward with activation
            self.stage_2_layer = DilatedConv1dBlock(embed_dim, embed_dim, enc_conv_kernel, 
                                                         enc_conv_dilation, single_conv=False)
        else:
            # Feed-forward layer with activation
            self.stage_2_layer = nn.Sequential(nn.Linear(embed_dim, embed_dim), nn.GELU())

        if self.norm:
            self.norm_layer_1 = nn.LayerNorm(embed_dim)
            self.norm_layer_2 = nn.LayerNorm(embed_dim)

        self.residual = residual

    def set_layer_idx(self, idx):
        self.layer_idx = idx

    def forward(self, x, inference_params=None):
        
        if self.norm:
            x = self.norm_layer_1(x)

        x_ssm = self.mamba_layer(x, inference_params=inference_params)

        if self.bidirectional_mamba:
            if inference_params is not None:
                raise NotImplementedError("Bidirectional Mamba with inference params is not implemented.")
            x_ssm = x_ssm + self.mamba_layer_backward(x.flip(dims=[1])).flip(dims=[1])

        if self.residual:
            x = x + x_ssm
        else:
            x = x_ssm

        if self.norm:
            x_out = self.stage_2_layer(self.norm_layer_2(x))
        else:
            x_out = self.stage_2_layer(x) 
        
        if self.residual:
            x_out = x_out + x

        return x_out
    
    def step_independent(self, xt, inference_params):
        """
        Process independent targets xt using the cached context from inference_params.
        
        Args:
            xt: (Batch, Num_Targets, Dim)
            inference_params: Cache containing the context state.
        """
        batch, nt, _ = xt.shape

        if self.norm:
            x = self.norm_layer_1(xt)
        else:
            x = xt

        cached_conv, cached_ssm = inference_params.key_value_memory_dict[self.layer_idx]
        
        # Reshape input to "Super Batch": (B*Nt, 1, D)
        x_mamba_in = einops.rearrange(x, "b nt d -> (b nt) 1 d")
        
        # Repeat States: (B, D, State) -> (B*Nt, D, State)
        # This creates independent copies for every target.
        conv_state = einops.repeat(cached_conv, "b ... -> (b nt) ...", nt=nt).contiguous()
        ssm_state = einops.repeat(cached_ssm, "b ... -> (b nt) ...", nt=nt).contiguous()

        # Step the mamba layer separately for each target point
        out_flat, _, _ = self.mamba_layer.step(x_mamba_in, conv_state, ssm_state)
        
        # Reshape back: (B, Nt, D)
        x_ssm = einops.rearrange(out_flat, "(b nt) 1 d -> b nt d", b=batch, nt=nt)

        if self.residual:
            x = x + x_ssm
        else:
            x = x_ssm

        if self.norm:
            x_out = self.stage_2_layer(self.norm_layer_2(x))
        else:
            x_out = self.stage_2_layer(x)

        if self.residual:
            x_out = x_out + x

        return x_out